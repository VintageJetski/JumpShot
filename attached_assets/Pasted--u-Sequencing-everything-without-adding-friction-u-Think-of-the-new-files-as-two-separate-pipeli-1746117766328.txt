<u>Sequencing everything without adding friction</u>
→ Think of the new files as two separate pipelines that meet in the middle – an offline “update my numbers” batch and an always-on API. You can trigger the batch pieces manually at first (one command each) and fold them into a lightweight scheduler once you are happy they run clean; the API file belongs in your normal startup flow.

<u>Where manual commands are safest while you test</u>
→ python clean.py whenever you drop fresh raw CSVs into /raw_events.
→ python metrics/core.py straight after; it enriches the parquet in /clean.
→ python metrics/piv.py to recalc PIV_v14 and write clean/piv.parquet.
→ python metrics/learn_weights.py only when you have updated team-result rows in match_results.csv; it spits out clean/learned_weights.csv for the UI.

<u>How to automate once you trust the run-order</u>
→ Add a tiny shell wrapper called refresh.sh:

bash
Copy
Edit
#!/usr/bin/env bash
python clean.py &&
python -m metrics.core &&
python -m metrics.piv &&
python -m metrics.learn_weights
mark it executable (chmod +x refresh.sh) and run ./refresh.sh in Replit’s shell or from any CI job.

→ If you want Replit to kick that refresh each time files appear, add a watch.sh that sleeps 60 s, checks ls -1 raw_events/*.csv | md5sum against a cached hash and calls refresh.sh when the checksum changes. Run watch.sh & at the top of your main start script so it hums away in the background.

<u>Integrating the live endpoints</u>
→ insights/api.py should import your existing Flask app and register its routes, or you can make it the entry file that builds the app, then call flask run under Replit’s .replit run command. Either way the server boots once and stays up; the batch scripts never need to run at the same time.

<u>Keeping startup clean</u>
→ Leave your normal main.py (or the file Replit already starts) unchanged except for one line that does from insights.api import attach_routes (or similar) so the new endpoints become part of the same app object.
→ Don’t hook clean.py etc. into that startup path; they run in their own refresh cycle and write parquet / CSV outputs your API reads lazily.

<u>Next-step niceties</u>
→ When you are ready to stop manual button-pushing, use Replit’s “Deployments → Reserved VM” to add a cron-style job that calls refresh.sh at 04:00 UTC daily, or set a GitHub Action in the repo that triggers on a push containing new event data.
→ Because each script is idempotent and atomic you do not need locks; if the scheduler fires while someone happens to kick it manually the second call just overwrites the same parquet.

That pattern keeps your real-time UI fast (only reading finished parquet) while batch updates remain simple, testable, and easy to roll back.